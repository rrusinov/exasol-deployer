terraform {
  required_version = ">= 1.0"
  required_providers {
    libvirt = {
      source  = "dmacvicar/libvirt"
      version = "= 0.7.6"
    }
    local = {
      source  = "hashicorp/local"
      version = "~> 2.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "~> 4.0"
    }
    random = {
      source  = "hashicorp/random"
      version = "~> 3.0"
    }
  }
}

locals {
  provider_name = "Libvirt"
  provider_code = "libvirt"
  region_name   = "local"
  libvirt_uri = trimspace(var.libvirt_uri)

  node_volumes = {
    for node_idx in range(var.node_count) : node_idx => [
      for vol_idx in range(var.data_volumes_per_node) :
      libvirt_volume.data_volume[node_idx * var.data_volumes_per_node + vol_idx].id
    ]
  }

  node_public_ips  = [for idx, domain in libvirt_domain.exasol_node : try(domain.network_interface[0].addresses[0], "")] 
  node_private_ips = [for idx, domain in libvirt_domain.exasol_node : try(domain.network_interface[0].addresses[0], "")] 
}

provider "libvirt" {
  uri = local.libvirt_uri
}



# For macOS HVF we avoid cloud-init ISO and instead attach a small configdrive volume per node.

resource "null_resource" "make_configdrive_iso" {
  count = var.node_count
  triggers = {
    user_data = local.cloud_init_script
    index     = count.index
  }

  provisioner "local-exec" {
    command = <<-EOT
      set -euo pipefail
      cd "${path.module}"
      mkdir -p .cloudinit/tmp/n${count.index + 11}
      cat > .cloudinit/tmp/n${count.index + 11}/user-data <<'UD'
    ${local.cloud_init_script}
    UD
      cat > .cloudinit/tmp/n${count.index + 11}/meta-data <<'MD'
    instance-id: n${count.index + 11}
    local-hostname: n${count.index + 11}
    MD
      if command -v genisoimage >/dev/null 2>&1; then
        genisoimage -output .cloudinit/n${count.index + 11}-configdrive.iso -volid cidata -joliet -rock .cloudinit/tmp/n${count.index + 11}/user-data .cloudinit/tmp/n${count.index + 11}/meta-data
      elif command -v mkisofs >/dev/null 2>&1; then
        mkisofs -o .cloudinit/n${count.index + 11}-configdrive.iso -V cidata -J -r .cloudinit/tmp/n${count.index + 11}/user-data .cloudinit/tmp/n${count.index + 11}/meta-data
      else
        echo "Neither genisoimage nor mkisofs found; please install one to generate configdrive ISOs" >&2
        exit 1
      fi
      rm -rf .cloudinit/tmp/n${count.index + 11}
    EOT
  }
}

resource "libvirt_volume" "configdrive" {
  count = var.node_count
  name  = "configdrive-n${count.index + 11}-${random_id.instance.hex}.img"
  pool  = var.libvirt_disk_pool
  format = "raw"
  # Use the ISO generated by the null_resource as the source for the volume
  source = "${path.module}/.cloudinit/n${count.index + 11}-configdrive.iso"
  depends_on = [null_resource.make_configdrive_iso]
}

# Also generate cloud-init ISOs from the shared cloud_init_script with optional static IPs
resource "libvirt_cloudinit_disk" "commoninit" {
  count = var.node_count
  name  = "commoninit-n${count.index + 11}-${random_id.instance.hex}.iso"
  pool  = var.libvirt_disk_pool
  # Use the shared cloud_init_script from templates/terraform-common/common.tf
  user_data = local.cloud_init_script
}

# Copy rest of main.tf resources except macOS-incompatible XSLT and cloudinit-iso usage

resource "libvirt_volume" "ubuntu_base" {
  name   = "ubuntu-base-${random_id.instance.hex}.qcow2"
  pool   = var.libvirt_disk_pool
  source = "https://cloud-images.ubuntu.com/releases/24.04/release/ubuntu-24.04-server-cloudimg-${var.instance_architecture == "arm64" ? "arm64" : "amd64"}.img"
  format = "qcow2"
  lifecycle {
    ignore_changes = [source]
  }
}

resource "libvirt_volume" "root_volume" {
  count          = var.node_count
  name           = "n${count.index + 11}-root-${random_id.instance.hex}.qcow2"
  pool           = var.libvirt_disk_pool
  base_volume_id = libvirt_volume.ubuntu_base.id
  size           = var.root_volume_size * 1073741824
}

resource "libvirt_volume" "data_volume" {
  count = var.node_count * var.data_volumes_per_node
  name  = "n${floor(count.index / var.data_volumes_per_node) + 11}-data-${(count.index % var.data_volumes_per_node) + 1}-${random_id.instance.hex}.qcow2"
  pool  = var.libvirt_disk_pool
  size  = var.data_volume_size * 1073741824
}

resource "libvirt_domain" "exasol_node" {
  count    = var.node_count
  name     = "n${count.index + 11}-${random_id.instance.hex}"
  type     = var.libvirt_domain_type
  machine  = "virt"
  memory   = var.libvirt_memory_gb * 1024
  vcpu     = var.libvirt_vcpus
  running  = var.infra_desired_state == "stopped" ? false : true

  # CPU configuration: macOS HVF uses default CPU mode (no host-passthrough)

  dynamic "network_interface" {
    for_each = var.libvirt_network_bridge != "" ? [1] : []
    content {
      network_name   = var.libvirt_network_bridge
      wait_for_lease = true
    }
  }

  dynamic "network_interface" {
    for_each = var.libvirt_network_bridge == "" ? [1] : []
    content {
      # Ensure libvirt reports guest IP when using user-mode networking (slirp)
      wait_for_lease = true
    }
  }

  # Attach configdrive volume (small raw image containing cloud-init userdata)
  disk {
    volume_id = libvirt_volume.configdrive[count.index].id
  }

  disk {
    volume_id = libvirt_volume.root_volume[count.index].id
  }

  dynamic "disk" {
    for_each = range(var.data_volumes_per_node)
    content {
      volume_id = libvirt_volume.data_volume[count.index * var.data_volumes_per_node + disk.value].id
    }
  }

  console {
    type        = "pty"
    target_type = "serial"
    target_port = "0"
  }

  graphics {
    type        = "spice"
    listen_type = "address"
    autoport    = true
  }

  provisioner "local-exec" {
    command = "sleep 30"
  }
}
